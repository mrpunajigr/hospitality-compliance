import { NextRequest, NextResponse } from 'next/server'
import { createClient } from '@supabase/supabase-js'

// Create admin client for server operations
const supabaseAdmin = createClient(
  process.env.NEXT_PUBLIC_SUPABASE_URL || '',
  process.env.SUPABASE_SERVICE_ROLE_KEY || '',
  {
    auth: {
      autoRefreshToken: false,
      persistSession: false
    }
  }
)

interface BulkProcessRequest {
  files: File[]
  clientId: string
  userId: string
  processingPriority?: 'high' | 'medium' | 'low'
  batchSize?: number
}

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData()
    const clientId = formData.get('clientId') as string
    const userId = formData.get('userId') as string
    const processingPriority = (formData.get('processingPriority') as string) || 'medium'
    const batchSize = parseInt(formData.get('batchSize') as string) || 10

    if (!clientId || !userId) {
      return NextResponse.json({ 
        error: 'Missing required fields: clientId or userId' 
      }, { status: 400 })
    }

    // Get all uploaded files
    const files: File[] = []
    const entries = Array.from(formData.entries())
    for (const [key, value] of entries) {
      if (key.startsWith('file_') && value instanceof File) {
        files.push(value)
      }
    }

    if (files.length === 0) {
      return NextResponse.json({ 
        error: 'No files provided for processing' 
      }, { status: 400 })
    }

    console.log(`ðŸš€ Starting bulk processing of ${files.length} dockets`)

    const results = {
      total: files.length,
      processed: 0,
      uploaded: 0,
      failed: 0,
      errors: [] as string[],
      deliveryRecords: [] as string[]
    }

    // Process files in batches to avoid overwhelming the system
    for (let i = 0; i < files.length; i += batchSize) {
      const batch = files.slice(i, i + batchSize)
      
      console.log(`ðŸ“¦ Processing batch ${Math.floor(i / batchSize) + 1} (${batch.length} files)`)
      
      // Process batch concurrently
      const batchPromises = batch.map(async (file, batchIndex) => {
        const fileIndex = i + batchIndex
        
        try {
          console.log(`ðŸ“„ Processing file ${fileIndex + 1}/${files.length}: ${file.name}`)

          // Create organized file path with proper date structure
          const timestamp = Date.now() + fileIndex // Ensure uniqueness
          const dateFolder = new Date().toISOString().split('T')[0] // YYYY-MM-DD
          const fileName = `bulk_${timestamp}_${file.name.replace(/[^a-zA-Z0-9.-]/g, '_')}`
          const filePath = `${clientId}/${dateFolder}/${fileName}` // Use client ID folder structure
          
          console.log(`ðŸ“‚ Upload path details:`)
          console.log(`   â€¢ Client ID: ${clientId}`)
          console.log(`   â€¢ Date folder: ${dateFolder}`)
          console.log(`   â€¢ File name: ${fileName}`)
          console.log(`   â€¢ Full path: ${filePath}`)
          console.log(`   â€¢ Original name: ${file.name}`)

          // Convert File to Buffer
          const buffer = Buffer.from(await file.arrayBuffer())

          // Upload to Supabase storage
          const { data: uploadResult, error: uploadError } = await supabaseAdmin.storage
            .from('delivery-dockets')
            .upload(filePath, buffer, {
              contentType: file.type,
              cacheControl: '3600',
              upsert: false
            })

          if (uploadError) {
            console.error(`âŒ Upload failed for ${file.name}:`, uploadError.message)
            results.failed++
            results.errors.push(`${file.name}: ${uploadError.message}`)
            return null
          }

          console.log(`âœ… Uploaded: ${file.name} -> ${filePath}`)
          console.log(`ðŸ“ Upload result:`, JSON.stringify(uploadResult, null, 2))
          results.uploaded++
          
          // Verify file was actually uploaded by checking bucket
          const { data: verifyData, error: verifyError } = await supabaseAdmin.storage
            .from('delivery-dockets')
            .list(filePath.split('/').slice(0, -1).join('/'))
          
          if (verifyData) {
            const fileName = filePath.split('/').pop()
            const fileExists = verifyData.some(f => f.name === fileName)
            console.log(`ðŸ” File verification: ${fileExists ? 'EXISTS' : 'MISSING'} in bucket after upload`)
          }

          // Call the real Supabase Edge Function for Document AI processing
          console.log(`ðŸ¤– Processing with AI: ${file.name}`)
          console.log(`ðŸ”— Edge Function URL: ${process.env.NEXT_PUBLIC_SUPABASE_URL}/functions/v1/process-delivery-docket`)
          console.log(`ðŸ”‘ Using Service Role Key: ${process.env.SUPABASE_SERVICE_ROLE_KEY ? 'AVAILABLE' : 'MISSING'}`)
          
          const edgeFunctionUrl = `${process.env.NEXT_PUBLIC_SUPABASE_URL}/functions/v1/process-delivery-docket`
          const requestBody = {
            bucketId: 'delivery-dockets',
            fileName: fileName,
            filePath: uploadResult.path,
            userId: userId,
            clientId: clientId,
            metadata: {
              bulkUpload: true,
              originalFileName: file.name,
              processingPriority: processingPriority,
              batchIndex: fileIndex
            }
          }
          
          console.log(`ðŸ“¤ Edge Function request body:`, JSON.stringify(requestBody, null, 2))
          console.log(`ðŸ”‘ Using Service Role Key: ${process.env.SUPABASE_SERVICE_ROLE_KEY ? 'SET (length: ' + process.env.SUPABASE_SERVICE_ROLE_KEY.length + ')' : 'MISSING'}`)
          
          // Add timeout for Google Cloud AI processing
          const controller = new AbortController()
          const timeoutId = setTimeout(() => controller.abort(), 180000) // 3 minute timeout for large documents
          
          const processingResponse = await fetch(edgeFunctionUrl, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${process.env.SUPABASE_SERVICE_ROLE_KEY}`,
            },
            body: JSON.stringify(requestBody),
            signal: controller.signal
          })
          
          clearTimeout(timeoutId) // Clear timeout if request completes
          
          console.log(`ðŸ“¥ Edge Function response status: ${processingResponse.status}`)
          console.log(`ðŸ“¥ Edge Function response headers:`, Object.fromEntries(processingResponse.headers.entries()))

          if (!processingResponse.ok) {
            const errorText = await processingResponse.text()
            console.error(`ðŸ¤– Edge Function failed for ${file.name}:`)
            console.error(`   Status: ${processingResponse.status}`)
            console.error(`   Response: ${errorText}`)
            console.error(`   URL: ${edgeFunctionUrl}`)
            
            // Return the error to frontend so we can see it
            return NextResponse.json({
              success: false,
              error: 'Edge Function failed',
              details: {
                status: processingResponse.status,
                response: errorText,
                url: edgeFunctionUrl,
                fileName: file.name
              }
            }, { status: 500 })
            
            // Create basic record even if AI processing fails
            const { data: deliveryRecord, error: dbError } = await supabaseAdmin
              .from('delivery_records')
              .insert({
                client_id: clientId,
                user_id: userId,
                image_path: uploadResult?.path || `failed_upload_${file.name}`,
                processing_status: 'failed',
                error_message: `AI processing failed: ${errorText}`,
                raw_extracted_text: 'Bulk upload - AI processing failed',
                metadata: {
                  bulkUpload: true,
                  originalFileName: file.name
                }
              })
              .select()
              .single()
              
            if (!dbError && deliveryRecord) {
              results.deliveryRecords.push(deliveryRecord.id)
              results.processed++
            }
            
            results.errors.push(`${file.name}: AI processing failed - ${errorText}`)
            return deliveryRecord?.id || null
          }

          // Validate JSON response before parsing
          const responseText = await processingResponse.text()
          console.log(`ðŸ“„ Raw Edge Function response for ${file.name}:`, responseText.substring(0, 500), '...')
          
          let processingResult
          try {
            processingResult = JSON.parse(responseText)
            console.log(`ðŸ¤– AI processing completed for ${file.name}:`, processingResult)
          } catch (jsonError) {
            console.error(`âŒ JSON parsing failed for ${file.name}:`, jsonError)
            console.error(`âŒ Raw response (first 1000 chars):`, responseText.substring(0, 1000))
            results.errors.push(`${file.name}: Invalid JSON response from Edge Function`)
            return null
          }

          // Find the created delivery record (with retry for timing issues)
          let deliveryRecord = null
          let retries = 3
          
          while (retries > 0 && !deliveryRecord) {
            const { data, error } = await supabaseAdmin
              .from('delivery_records')
              .select('id')
              .eq('image_path', uploadResult.path)
              .single()
              
            if (data) {
              deliveryRecord = data
            } else {
              console.log(`â³ Delivery record not found yet, retrying... (${retries} attempts left)`)
              await new Promise(resolve => setTimeout(resolve, 1000)) // Wait 1 second
              retries--
            }
          }

          if (deliveryRecord) {
            results.deliveryRecords.push(deliveryRecord.id)
            results.processed++
            console.log(`âœ… Successfully processed ${file.name} - Record ID: ${deliveryRecord.id}`)
          } else {
            console.error(`âŒ Could not find delivery record for ${file.name} after AI processing`)
            results.errors.push(`${file.name}: Record not found after AI processing`)
          }
          
          // Create audit log for bulk processing (only if record found)
          if (deliveryRecord) {
            await supabaseAdmin
              .from('audit_logs')
              .insert({
                client_id: clientId,
                user_id: userId,
                action: 'bulk.document.processed',
                resource_type: 'delivery_record',
                resource_id: deliveryRecord.id,
                details: {
                  fileName: file.name,
                  originalFileName: file.name,
                  bulkUpload: true,
                  processingPriority,
                  batchIndex: fileIndex,
                  aiProcessingSuccess: true
                }
              })
          }

          return deliveryRecord?.id || null

        } catch (error) {
          console.error(`âŒ Error processing ${file.name}:`, error)
          results.failed++
          
          if (error instanceof Error && error.name === 'AbortError') {
            console.error(`â° Timeout processing ${file.name} - Google Cloud AI took too long`)
            results.errors.push(`${file.name}: Processing timeout (Google Cloud AI took >3 minutes)`)
          } else {
            const errorMessage = error instanceof Error ? error.message : 'Unknown error'
            results.errors.push(`${file.name}: ${errorMessage}`)
          }
          return null
        }
      })

      // Wait for batch to complete before starting next batch
      const batchResults = await Promise.all(batchPromises)
      
      console.log(`âœ… Batch ${Math.floor(i / batchSize) + 1} completed`)
      
      // Small delay between batches to avoid overwhelming the system
      if (i + batchSize < files.length) {
        await new Promise(resolve => setTimeout(resolve, 1000))
      }
    }

    console.log(`ðŸŽ‰ Bulk processing completed:`)
    console.log(`   ðŸ“Š Total: ${results.total}`)
    console.log(`   âœ… Processed: ${results.processed}`)
    console.log(`   ðŸ“¤ Uploaded: ${results.uploaded}`)
    console.log(`   âŒ Failed: ${results.failed}`)

    // Create bulk processing summary audit log
    await supabaseAdmin
      .from('audit_logs')
      .insert({
        client_id: clientId,
        user_id: userId,
        action: 'bulk.processing.completed',
        resource_type: 'bulk_operation',
        resource_id: null,
        details: {
          totalFiles: results.total,
          processedFiles: results.processed,
          uploadedFiles: results.uploaded,
          failedFiles: results.failed,
          processingPriority,
          batchSize,
          deliveryRecordIds: results.deliveryRecords,
          errors: results.errors
        }
      })

    return NextResponse.json({
      success: true,
      message: `Bulk processing completed: ${results.processed}/${results.total} files processed successfully`,
      results
    })

  } catch (error) {
    console.error('âŒ Bulk processing error:', error)
    console.error('âŒ Error stack:', error instanceof Error ? error.stack : 'No stack trace')
    console.error('âŒ Error details:', JSON.stringify(error, Object.getOwnPropertyNames(error), 2))
    
    const errorMessage = error instanceof Error ? error.message : 'Unknown error'
    const errorResponse = { 
      error: 'Bulk processing failed', 
      details: errorMessage,
      stack: error instanceof Error ? error.stack : 'No stack trace'
    }
    
    console.log('ðŸ” Sending error response:', JSON.stringify(errorResponse, null, 2))
    
    return NextResponse.json(errorResponse, { status: 500 })
  }
}

export async function GET(request: NextRequest) {
  try {
    const url = new URL(request.url)
    const clientId = url.searchParams.get('clientId')
    
    if (!clientId) {
      return NextResponse.json({ error: 'Client ID required' }, { status: 400 })
    }

    // Get bulk processing statistics
    const { data: bulkRecords, error } = await supabaseAdmin
      .from('delivery_records')
      .select('id, processing_status, confidence_score, created_at, metadata')
      .eq('client_id', clientId)
      .not('metadata', 'is', null)
      .order('created_at', { ascending: false })

    if (error) {
      return NextResponse.json({ error: error.message }, { status: 500 })
    }

    const bulkUploads = bulkRecords?.filter(record => 
      record.metadata && record.metadata.bulkUpload === true
    ) || []

    const stats = {
      totalBulkRecords: bulkUploads.length,
      completedProcessing: bulkUploads.filter(r => r.processing_status === 'completed').length,
      failedProcessing: bulkUploads.filter(r => r.processing_status === 'failed').length,
      averageConfidence: bulkUploads.filter(r => r.confidence_score).length > 0
        ? (bulkUploads.reduce((sum, r) => sum + (r.confidence_score || 0), 0) / 
           bulkUploads.filter(r => r.confidence_score).length)
        : 0,
      lastBulkUpload: bulkUploads.length > 0 ? bulkUploads[0].created_at : null,
      readyForTraining: bulkUploads.filter(r => 
        r.processing_status === 'completed' && (r.confidence_score || 0) < 0.85
      ).length
    }

    return NextResponse.json({ stats, recentUploads: bulkUploads.slice(0, 20) })

  } catch (error) {
    console.error('Bulk processing GET error:', error)
    return NextResponse.json({ error: 'Internal server error' }, { status: 500 })
  }
}